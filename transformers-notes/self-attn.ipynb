{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77d8124-2a76-4472-9490-a3ea699e750d",
   "metadata": {},
   "source": [
    "# Self Attention Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3bcbdb-6aea-46d2-8a2d-e67bfe70a1c2",
   "metadata": {},
   "source": [
    "One thing we want to do now is to better approximate $\\mathbb{P}(x_1, x_2, ..., x_n)$. The goal of this notebook is to motivate and familiarize you with the mechanism that powers transformers: self-attention. But before we do that, we need to understand what *embeddings* are."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577589b-63de-40a2-a961-099e1b091ea3",
   "metadata": {},
   "source": [
    "## Embeddings / Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4189b6-6308-462e-8968-03d7ef425c8e",
   "metadata": {},
   "source": [
    "In the neural networks context, embeddings are vectors which represent a data point or piece of information. In the NLP case, we can think of embeddings as being \"word vectors\" or \"token vectors.\" In the context of this project, we will assign tokens integer IDs. I.e. the letter \"a\" might be assigned to ID 0. We can think of an \"embedding table\" as a function that maps these integers to n-dimensional vectors, i.e.\n",
    "$$f: \\mathbb{Z^+} \\rightarrow \\mathbb{R}^n$$\n",
    "\n",
    "Typically, **the embeddings are much higher dimensional than the vocabulary**. This is important as we'll see later. Quick aside: I'll generally use the terms embeddings and representations interchangably. But the *embeddings layer* is the embedding lookup table that creates the initial representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1215,
   "id": "ee7d59ba-33e1-4ed1-b032-55e6f86efa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz .\"\n",
    "\n",
    "vocab_size = len(alphabet)\n",
    "embed_dim = 512\n",
    "txt2idx = {alphabet[i]:i for i in range(len(alphabet))}\n",
    "idx2txt = {i:alphabet[i] for i in range(len(alphabet))} \n",
    "\n",
    "embedding_table = np.random.normal(size=(vocab_size, embed_dim))\n",
    "def embed_token(token):\n",
    "    token_idx = txt2idx[token]\n",
    "    return embedding_table[token_idx]\n",
    "\n",
    "def embed_sentence(sentence):\n",
    "    out = []\n",
    "    for t in sentence:\n",
    "        out.append(\n",
    "            embed_token(t)\n",
    "        )\n",
    "    return np.array(out)\n",
    "\n",
    "embeddings = embed_sentence(\"hey sean. whats up\")\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fc8fea-733d-4fe5-acba-32b912cce79d",
   "metadata": {},
   "source": [
    "By converting\n",
    "* text -> tokens\n",
    "* tokens -> integers\n",
    "* integers -> embeddings\n",
    "\n",
    "we effectively have a function that can map a sentence to a sequence of vectors.\n",
    " \n",
    "### Aside on embedding size and information:\n",
    "If we have a vocabulary of ~64k, we need ~16 bits = 2 bytes of information to express each word in our vocabulary. Or, in other word, each word can be expressed by a 16-dimensional one-hot vector. By using embeddings (say at fp16 precision and 512 dimensional), each word goes from needing 2 bytes to represent to needing 1024 bytes of information. One reason we require more bytes is because of the higher dimensionality, and the other is the switch from a discrete representation to a continuous representation.\n",
    "\n",
    "Why do we do this? Why do we represent words in neural networks with so much redundancy? Consider all the cases where the word \"the\" might be used. In one sentence the word \"the\" can take on multiple contexts -- ideally, the \"true\" embeddings of a token or word (or character) should correspond to a **contextualized** representation of that word. So choosing to represent words as high dimensional vectors gives our model a lot of room to change the vector when contextualized."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd40b0e-1f14-4791-a153-5ed87d96f2f6",
   "metadata": {},
   "source": [
    "## How do we use representations for language modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4849eb2a-ecf0-45b1-82b1-5e86dfb97e0a",
   "metadata": {},
   "source": [
    "A natural question that arises is: how are embeddings useful in practice? I.e. how do they work within a Transformer, and how do they contribute to learning $\\mathbb{P}$?\n",
    "\n",
    "Consider the below code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1216,
   "id": "58aa3c34-1c19-410a-b272-07286db4315d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  7.7038926 , -23.38859578,   4.08575033,  32.75940082,\n",
       "        -2.46142945,   1.17977083,   2.80594505, -19.73685707,\n",
       "        -5.67724779, -12.9283562 ,  -5.82059853,  14.05219237,\n",
       "        -6.69960456,  19.01724971,   4.13239048,  16.48436548,\n",
       "         8.10346048,  33.49901703,  18.96133537,   7.04676268,\n",
       "         1.37820658,  24.86854581,  -6.82997835,  29.901247  ,\n",
       "        27.4442641 , -24.14996626, -19.31185507,  34.02881432])"
      ]
     },
     "execution_count": 1216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_proj = np.random.normal(size=(embed_dim, vocab_size))\n",
    "\n",
    "embeddings[-1] @ out_proj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac4a1a6-ced1-4ad9-96b6-ff48277ac0f5",
   "metadata": {},
   "source": [
    "What I've constructed above is a **projection matrix**. This will take an embedding, and *project* it to another vector space -- in this case, back to the embedding dim. The idea behind these embeddings and internal representations is that, if they're good enough, we can project them back to the vocab space as a distribution over the next token. E.g. as part of our transformer language model, the final layer will be a matrix \n",
    "\n",
    "$$M: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{\\text{vocab}}$$\n",
    "\n",
    "which then gives us our probability distribution, and our language model. In other words, the goal of a Transformer is to learn good enough representations such that the correct next-token distribution $\\mathbb{P}$ is a linear function of our representations. In practice, we typically just project the *last* token in our sequence to get this, and I'll explain why that is shortly. Keep in mind that in order to ensure the output of the projection is a true distribution, we use something called *softmax* to normalize the distribution to add up to 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e8aa96-62f3-4327-a7d7-234a23e10f55",
   "metadata": {},
   "source": [
    "![](trans.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbda10f-d2a6-4ed9-b072-135718932edc",
   "metadata": {},
   "source": [
    "Now currently, both our embedding table and projection matrix aren't particularly useful -- they're both randomly initialized. But the point is that with enough data, we can update these such that they work to make useful predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27e2c4c-2777-41ed-b663-c1213cecebce",
   "metadata": {},
   "source": [
    "## The Training Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcdcc4a-bdb2-4595-90f6-4a2e2e952177",
   "metadata": {},
   "source": [
    "How do neural networks learn from data? In the simplest terms, they see tons of examples, and use something called a **loss function** to tell the network how good its predictions were. It then uses calculus to figure out how to minimize this loss function by updating its weights. In the above case, our simple neural network would be updating the embedding table and the projection matrix to minimize the loss.\n",
    "\n",
    "So what loss do Language Models use? They use something called **cross entropy**:\n",
    "\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} -ln(y_i)$$\n",
    "\n",
    "with $y_i$ being the **likelihood of the correct token as predicted by the model**.\n",
    "\n",
    "For example, if my language model assigns the token \"j\" a likelihood of $0.03$ but it was the correct token given the context, we would say that the loss for that example is $-ln(0.03) \\approx 3.5$. Minimizing this loss corresponds with maximizing the likelihood of the correct token $y_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb07a9e-27ed-4eee-8784-34319c19f389",
   "metadata": {},
   "source": [
    "Using the embedding table, proj matrix, softmax, and cross entropy, I'll show a brief implementation of training this rudimentary network in pytorch using the tiny shakespeare dataset. This isn't the full Transformer (it's actually something we've covered before!), but this is the skeleton of what mechanistically makes it work. After this section we'll discuss the crux of this lecture: self-attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1217,
   "id": "c4933f56-cd6d-4014-9143-237fe9026c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab: ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# lots of code adapted from the goat Andrej karpathy\n",
    "torch.manual_seed(42)\n",
    "\n",
    "with open('tinyshakespeare.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print(f'Vocab: {chars}')\n",
    "print(f'Vocab size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1218,
   "id": "a3469e72-f355-4fff-a871-6bdd60a611ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[46, 47, 1, 58, 46, 43, 56, 43, 2]\n",
      "hi there!\n"
     ]
    }
   ],
   "source": [
    "# Prepare the dataset\n",
    "# create a mapping from characters to integers\n",
    "txt2idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx2txt = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [txt2idx[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([idx2txt[i] for i in l]) # decoder: take a list of integers, output a string\n",
    "\n",
    "print(encode(\"hi there!\"))\n",
    "print(decode(encode(\"hi there!\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1221,
   "id": "2f84fcb9-7984-4a7f-a05a-0fa901a081ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxLM(nn.Module):\n",
    "    def __init__(self, vocab_size, channel_dim=64):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, channel_dim)\n",
    "        self.proj = nn.Linear(channel_dim, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        logits = self.proj(self.token_embedding_table(\n",
    "            idx\n",
    "        )) # (B, T, C)\n",
    "        \n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "    \n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1222,
   "id": "e4c241a0-55c6-4e47-a771-6f5ea09772e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sample: tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])\n",
      "when input is tensor([18]) the target: 47\n",
      "when input is tensor([18, 47]) the target: 56\n",
      "when input is tensor([18, 47, 56]) the target: 57\n",
      "when input is tensor([18, 47, 56, 57]) the target: 58\n",
      "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
      "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
      "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
     ]
    }
   ],
   "source": [
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "\n",
    "block_size = 8\n",
    "print(f'Example sample: {train_data[:block_size+1]}')\n",
    "\n",
    "x = train_data[:block_size]\n",
    "y = train_data[1:block_size+1]\n",
    "for t in range(block_size):\n",
    "    context = x[:t+1]\n",
    "    target = y[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1223,
   "id": "22c7ff76-fd66-448d-940b-6eeb642fcdf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
      "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
      "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
      "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
      "----\n",
      "when input is [24] the target: 43\n",
      "when input is [24, 43] the target: 58\n",
      "when input is [24, 43, 58] the target: 5\n",
      "when input is [24, 43, 58, 5] the target: 57\n",
      "when input is [24, 43, 58, 5, 57] the target: 1\n",
      "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
      "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
      "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
      "when input is [44] the target: 53\n",
      "when input is [44, 53] the target: 56\n",
      "when input is [44, 53, 56] the target: 1\n",
      "when input is [44, 53, 56, 1] the target: 58\n",
      "when input is [44, 53, 56, 1, 58] the target: 46\n",
      "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
      "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
      "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52] the target: 58\n",
      "when input is [52, 58] the target: 1\n",
      "when input is [52, 58, 1] the target: 58\n",
      "when input is [52, 58, 1, 58] the target: 46\n",
      "when input is [52, 58, 1, 58, 46] the target: 39\n",
      "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
      "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
      "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
      "when input is [25] the target: 17\n",
      "when input is [25, 17] the target: 27\n",
      "when input is [25, 17, 27] the target: 10\n",
      "when input is [25, 17, 27, 10] the target: 0\n",
      "when input is [25, 17, 27, 10, 0] the target: 21\n",
      "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
      "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
      "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1337)\n",
    "batch_size = 4 # how many independent sequences will we process in parallel?\n",
    "block_size = 8 # what is the maximum context length for predictions?\n",
    "\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x, y\n",
    "\n",
    "xb, yb = get_batch('train')\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('----')\n",
    "\n",
    "for b in range(batch_size): # batch dimension\n",
    "    for t in range(block_size): # time dimension\n",
    "        context = xb[b, :t+1]\n",
    "        target = yb[b,t]\n",
    "        print(f\"when input is {context.tolist()} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1224,
   "id": "03199808-bfde-4f9d-97bd-a6dcb5a37da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
      "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
      "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
      "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
     ]
    }
   ],
   "source": [
    "print(xb) # our input to the transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1225,
   "id": "7fe17d2b-1abd-479a-a26f-40c5b0475d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 65])\n",
      "tensor(4.3428, grad_fn=<NllLossBackward0>)\n",
      "\n",
      "Z\n",
      "QiewEpPV.mLoRqApmSnAhXVK\n",
      "$hR\n",
      "fYkdFBUy-aMv,ieetsrJbc3k'ALxidKUy;RpPqAyHvs:TorvT?rVUC,he$\n",
      "NjeAAzX$tG\n"
     ]
    }
   ],
   "source": [
    "m = SoftmaxLM(vocab_size)\n",
    "logits, loss = m(xb, yb)\n",
    "print(logits.shape)\n",
    "print(loss)\n",
    "\n",
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1227,
   "id": "08812dbc-6d09-42b9-8bcf-b661af781b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4928877353668213\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "for steps in range(1000): # increase number of steps for good results... \n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1228,
   "id": "9c96c1a3-90bb-4279-8f28-3dfd63a53f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "A beoteamaill vousivecome nd by we apre--hocoongs me woucathey ifomexe layimeime: me thiatld, ps brerlitw'te. sids as ntinor wist. k hods, olde. dicenmerd ETou'd sout is.\n",
      "POrd beid fely owhte Thillles;\n",
      "core arwiefendind y t? mon bemanghirimy dime, thorarslf t.\n",
      "FFoumper of trse ttou, bad ght\n",
      "I my d!\n",
      "\n",
      "HWou ngremer at wo heaco f, wiventherieretevono tr as lyon le o!\n",
      "l s,\n",
      "Y pe'st\n",
      "Tr; mo-\n",
      "I ofamorltthace weand thother than tis sthd hosot.\n",
      "VI in hive G cho jufe o; t y atee southasl's? ind chivous dous\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6e5fc-c95b-4c62-aeb3-a4f05584d5a0",
   "metadata": {},
   "source": [
    "These results are pretty cool! But one problem with this current model is that **it doesn't contextualize anything** -- it simply takes the last token embedding and uses it to predict. In a sense, we've built a type of bigram with gradient descent instead of counting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2b536-da17-4128-bc85-a7c5e0581e95",
   "metadata": {},
   "source": [
    "![](soft.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ed8acd-f1b1-426f-b3f4-2ba58a22ae51",
   "metadata": {},
   "source": [
    "So how do we make this better? The answer is context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28973340-2b30-4dcf-9123-95269c10e1b8",
   "metadata": {},
   "source": [
    "## Contextualizing our Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6e6ddd-7b67-40df-961d-42393c88fd07",
   "metadata": {},
   "source": [
    "The missing piece from our current setup is that tokens don't *interact* with each other. In other words, our model doesn't take context into account. There is a static embedding table, and the embeddings of other things in the context dont influence the output. So now I'll try to motivate attention by discussing context. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b38ede-9fe5-4386-be9b-33862c70e27b",
   "metadata": {},
   "source": [
    "### Averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784845e3-bf5d-4898-a6e7-c604cb9544a8",
   "metadata": {},
   "source": [
    "To start, perhaps the simplest way to bake context into our model is by simply averaging all the embedding vectors before projecting. Currently, we simply pluck out the last embedding vector and project, but we can instead average out the sequence and then project the average. Let's train that and see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1229,
   "id": "8c9d9838-8cbd-4f70-998a-d640ffe55532",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLMWithAveraging(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, channel_dim=64):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, channel_dim)\n",
    "        self.proj = nn.Linear(channel_dim, vocab_size)\n",
    "        self.channel_dim = channel_dim\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        x = self.token_embedding_table(idx) # (B,T,C)\n",
    "        xbar = x.mean(axis=1, keepdims=True).repeat(1, T, 1) # (B, 1, C) -> (B, T, C)\n",
    "\n",
    "        logits = self.proj(xbar) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # print(probs)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "            # print(idx)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "id": "9205d001-a6de-4f6e-85ce-bbf4db34a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = SimpleLMWithAveraging(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "id": "e0de299f-a4e0-4a2c-b4fd-fef833ca2018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3625059127807617\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "for steps in range(1000): # increase number of steps for good results... \n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # print(xb.shape)\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1233,
   "id": "c14ae6a6-83c7-4e18-babe-1235336e481d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "U\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=50)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c212897-7097-4849-a59f-c468a75078d0",
   "metadata": {},
   "source": [
    "Quite sus unfortunately. We still have some issues here:\n",
    "* all averages all the same -- when we average and replace all the embeddings with the average over the sequence, we essentially predict the same thing for each token. this is bad.\n",
    "* tokens looking ahead -- the token at the beginning of the sequence averages with tokens after it. this is bad, as when we're generating, each token can't look past itself. So we need to change the averaging code slightly to ensure each vector only averages with things before it.\n",
    "* positional embeddings -- currently each word gets an embedding, but we don't have a way to indicate to the model which timestep each token is currently at. we can add this by just adding a small amount to each vector depending on its position\n",
    "* context-dependent averaging -- ideally, the average we compute should be *dependent on the token we're looking at*. We can try computing the dot product between each embedding and each other embedding -- this will allow the model to selectively decide weights for each element of the sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e273975a-dcce-4c4a-a5b9-d865ca55df86",
   "metadata": {},
   "source": [
    "Let's implement some architectural changes to alleviate this. The next iteration will have the following key features:\n",
    "* no token lookahead with *masking*\n",
    "* positional embeddings with another embedding table\n",
    "* context-dependent averaging with dot products"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e63b6b4-0f55-4d21-871b-e9a81aeb1ce5",
   "metadata": {},
   "source": [
    "### Quick aside: matmul as masking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022da00b-48fe-455b-b81f-349e4f65ccf0",
   "metadata": {},
   "source": [
    "Let's say we have some tensors of the shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "id": "b19217c0-946a-4b7f-89aa-8d825ebbe54f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6., 0., 9.],\n",
       "        [3., 3., 1.],\n",
       "        [3., 8., 9.]])"
      ]
     },
     "execution_count": 988,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R, C = 3, 3\n",
    "x = torch.randint(0,10,(R, C)).float()\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6774f9e6-3a12-4c4a-b39d-527f06148c15",
   "metadata": {},
   "source": [
    "If we want to average across the columns, and replace all the values, we can just compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "id": "9a63209b-1845-433a-a3e0-443a65b313fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0000, 3.6667, 6.3333],\n",
       "        [4.0000, 3.6667, 6.3333],\n",
       "        [4.0000, 3.6667, 6.3333]])"
      ]
     },
     "execution_count": 990,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(axis=0).repeat(R, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17dd2d8-448b-4e2d-a0c0-d8bffd1de478",
   "metadata": {},
   "source": [
    "But another way to do this is with matrix multiplication! Consider the following matmul:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "id": "c3101479-aaa7-4ed2-bb88-33d2e86f9023",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.3333, 0.3333],\n",
       "        [0.3333, 0.3333, 0.3333],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 991,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.ones(R, C)\n",
    "w = w / torch.sum(w, 1, keepdim=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "id": "301aa5fe-c653-4572-9213-5e9667fd4165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.0000, 3.6667, 6.3333],\n",
       "        [4.0000, 3.6667, 6.3333],\n",
       "        [4.0000, 3.6667, 6.3333]])"
      ]
     },
     "execution_count": 992,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w @ x # matrix mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b66e3d-eee5-4b8c-b4ae-1347b521eef3",
   "metadata": {},
   "source": [
    "Very cool! We can actually use this to our advantage though. Now consider the following weight matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "8bdf8bf1-4d97-4264-b3c5-c2349a299247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000],\n",
       "        [0.5000, 0.5000, 0.0000],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 994,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = torch.tril(torch.ones(R, C))\n",
    "w = w / torch.sum(w, 1, keepdim=True)\n",
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbf2b0e-c8ea-4275-8594-bcd5b4bd1ab6",
   "metadata": {},
   "source": [
    "When we compute the product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "id": "9147e3bd-19ae-486a-b794-df598adc0d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original mat: tensor([[6., 0., 9.],\n",
      "        [3., 3., 1.],\n",
      "        [3., 8., 9.]])\n",
      "no lookahead avg: tensor([[6.0000, 0.0000, 9.0000],\n",
      "        [4.5000, 1.5000, 5.0000],\n",
      "        [4.0000, 3.6667, 6.3333]])\n"
     ]
    }
   ],
   "source": [
    "print(f'original mat: {x}')\n",
    "print(f'no lookahead avg: {w @ x}') # matrix mul"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01de0f1-5307-4dc0-a644-1b3fac105711",
   "metadata": {},
   "source": [
    "We get the averaging we want without a lookahead! The first row is the same as in the original matrix, since we only average 1 thing. But in subsequent rows, our average only takes into account values in each column that's been previously encountered. We can use this trick to stop the averaging lookahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b184b0b6-facc-4822-b608-199c3319035b",
   "metadata": {},
   "source": [
    "![](attn_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1234,
   "id": "0bcb3ead-3b05-4778-8888-7c75ae1cc735",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLMWithBetterAveragingAndPosEmb(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, seqlen=8, channel_dim=64):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, channel_dim)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, channel_dim)\n",
    "        self.weights = nn.Linear(channel_dim, seqlen)\n",
    "        self.proj = nn.Linear(channel_dim, vocab_size)\n",
    "        self.channel_dim = channel_dim\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # we do positional embeddings now.\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        \n",
    "        # no lookahead trick + weighted average\n",
    "        wei = x @ x.transpose(1, 2) # compute weights for a weighted average based on each element of x dotting with itself\n",
    "        tril = torch.tril(torch.ones(T, T))  # remove weights for elements past the current element\n",
    "        wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1) # normalize weights to sum to 1\n",
    "        \n",
    "        xbar = wei @ x\n",
    "        logits = self.proj(xbar) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "    \n",
    "m = SimpleLMWithBetterAveragingAndPosEmb(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1236,
   "id": "c5841b4e-f59c-4141-9042-70d4d70860c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4606504440307617\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "for steps in range(1000): # increase number of steps for good results... \n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # print(xb.shape)\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1237,
   "id": "c3fd46fa-4e4c-4657-8b5a-02275d2b2a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nowsigng we f ire s d\n",
      "\n",
      "\n",
      "GTir:\n",
      "\n",
      "Wem, cetsou te w h sho a othorirer mefrere n stomullouetheipendlit tend he bese,\n",
      "Bllos I nk noke iendy pir deit poney t, shoind y, win isploy, y ache Thd otory, fonden he rtly to wor;\n",
      "Whithif her a as bestclfors anerure? ty t\n",
      "Hay ce m on; my is pad; ss ace.\n",
      "He bres w th fulithamed thethildiensprin nday shit'domo Pn.\n",
      "I'mat we cudse berequgh n? w I his anlithe becy. fest at bi'st sharonindHivence nind kee, me:\n",
      "PLUENEE:\n",
      "Be y epy choder blanchasetl m tir cureerdangas l\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e5a3e0-b176-499c-a3bd-0e3a1b697f60",
   "metadata": {},
   "source": [
    "If you understand this, you basically understand the mechanism behind what a Transformer does. There's one small change left, and then we will build our first real Transformer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ff88e9-35e5-487c-8e62-2df23c6178bf",
   "metadata": {},
   "source": [
    "### Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c1d992-9a96-4fcd-a6d6-560d275202d2",
   "metadata": {},
   "source": [
    "The last step to understanding self attention is to *break symmetry*. Note that the way we currently have it, the \n",
    "`wei = x @ x.transpose(1, 2)` is symmetric, i.e. x matmul'd with itself is the same regardless of the way you order the `x`'s (as long as the dimensions make sense). In self-attention, instead of plainly using the token embeddings, we compute *projections* of `x` into `queries, keys,` and `values`. In self-attention, the queries attend to the keys to compute our weights for the weighted average. And instead of averaging the embeddings themselves, we average over the `values` to get our final contextualized embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a166ea6-b6d5-4a20-8d39-c82045431b8a",
   "metadata": {},
   "source": [
    "Let's look at a small example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "id": "a6adcc83-4a0e-4c8e-ae08-92bec9bf368e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input seq: tensor([[-1.0394,  1.2911],\n",
      "        [-1.7285, -0.2545],\n",
      "        [-1.1180,  0.9540],\n",
      "        [ 2.9836,  0.6682]])\n"
     ]
    }
   ],
   "source": [
    "q = nn.Linear(2, 2)\n",
    "k = nn.Linear(2, 2)\n",
    "v = nn.Linear(2, 2)\n",
    "\n",
    "T, C = 4, 2\n",
    "inp = torch.randn((T,C)).float() # shape: T, C\n",
    "print(f'Input seq: {inp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "id": "2f5e3dd9-1b60-4e2f-8bd1-a3a8c146a3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Queries: tensor([[ 0.3488,  0.7321],\n",
      "        [-0.0761, -0.0067],\n",
      "        [ 0.2861,  0.5970],\n",
      "        [ 1.9730,  1.9966]], grad_fn=<AddmmBackward0>)\n",
      "Keys: tensor([[-0.9661, -0.4352],\n",
      "        [-0.1405, -0.8073],\n",
      "        [-0.7698, -0.4804],\n",
      "        [ 0.3425,  1.5715]], grad_fn=<AddmmBackward0>)\n",
      "Values: tensor([[ 0.6862,  1.2008],\n",
      "        [ 0.1113,  1.0918],\n",
      "        [ 0.5217,  1.1424],\n",
      "        [-1.8952, -0.9225]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "queries = q(inp)\n",
    "keys = k(inp)\n",
    "vals = v(inp)\n",
    "\n",
    "print(f'Queries: {queries}') # shape: T, C\n",
    "print(f'Keys: {keys}') # shape: T, C\n",
    "print(f'Values: {vals}') # shape: T, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1240,
   "id": "1c9c00c1-e518-48ca-b9be-d0372a688ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6556, -0.6401, -0.6202,  1.2700],\n",
      "        [ 0.0764,  0.0161,  0.0618, -0.0366],\n",
      "        [-0.5362, -0.5222, -0.5070,  1.0362],\n",
      "        [-2.7749, -1.8891, -2.4779,  3.8135]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "wei = queries @ keys.T # shape: T, T\n",
    "print(wei)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1241,
   "id": "82dc51cb-b788-42f5-aa2a-f8103ebbbcaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
       "        [0.5151, 0.4849, 0.0000, 0.0000],\n",
       "        [0.3285, 0.3332, 0.3383, 0.0000],\n",
       "        [0.0014, 0.0033, 0.0018, 0.9935]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 1241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tril = torch.tril(torch.ones(T, T))  # remove weights for elements past the current element\n",
    "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1) # normalize weights to sum to 1\n",
    "wei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1242,
   "id": "ebb76fa4-c97d-462f-94f4-ed87de1bc893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6862,  1.2008],\n",
       "        [ 0.4074,  1.1480],\n",
       "        [ 0.4390,  1.1447],\n",
       "        [-1.8805, -0.9091]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 1242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wei @ vals # new embeddings (shape T, C)! we can project this or repeat the process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84898ed9-82ea-4e86-acc4-b1ece50de951",
   "metadata": {},
   "source": [
    "## Practice: find the output embeddings (by hand) using self-attention for the following setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7ba438-eea3-4982-b7de-97429ece8a3b",
   "metadata": {},
   "source": [
    "$$T = 2, C = 2, X_{inp} = \\begin{bmatrix}1 & 0 \\\\ 1 & 1 \\end{bmatrix} \\in \\mathbb{R}^{TxC}$$\n",
    "$$Q_{proj} = \\begin{bmatrix}1 & 0 \\\\ 0 & 1 \\end{bmatrix}, K_{proj} = \\begin{bmatrix}1 & 1 \\\\ 0 & 1 \\end{bmatrix}, V_{proj} = \\begin{bmatrix}-1 & 1 \\\\ 0 & 3 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c56ba3-7073-4eb7-a81e-f2c3118eb70c",
   "metadata": {},
   "source": [
    "$$\\forall z \\in \\mathbb{R}^n, softmax(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^n e^{z_j}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3f8c9-d891-49e9-8c1f-3078b5985349",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba7c7bd-2fab-4535-83b2-7d007c6bba34",
   "metadata": {},
   "source": [
    "\n",
    "$$Q = X_{inp} Q_{proj}  = X_{inp} = \\begin{bmatrix} 1 & 0 \\\\ 1 & 1 \\end{bmatrix}$$\n",
    "$$K = X_{inp} K_{proj} = \\begin{bmatrix}1 & 1 \\\\ 1 & 2\\end{bmatrix}$$\n",
    "$$V = X_{inp} V_{proj} = \\begin{bmatrix}-1 & 1 \\\\ -1 & 4\\end{bmatrix}$$\n",
    "$$A = QK^T = \\begin{bmatrix} 1 & 1 \\\\ 2 & 3 \\end{bmatrix}$$\n",
    "$$W = \\text{softmax}(A \\cdot \\begin{bmatrix}1 & -inf \\\\ 1 & 1\\end{bmatrix})\\begin{bmatrix}1 & 0 \\\\ \\frac{e^2}{e^2 + e^3} & \\frac{e^3}{e^2 + e^3}\\end{bmatrix}$$\n",
    "$$X_{out} = WV = \\begin{bmatrix}-1 & 1 \\\\ -1 &  \\frac{e^2 + 4e^3}{e^2 + e^3}\\end{bmatrix} \\approx \\begin{bmatrix}-1 & 1 \\\\ -1 & 3.912\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "id": "9108eae5-6332-4cad-811e-63b8c8720ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q =  tensor([[1., 0.],\n",
      "        [1., 1.]])\n",
      "k =  tensor([[1., 1.],\n",
      "        [1., 2.]])\n",
      "v =  tensor([[-1.,  1.],\n",
      "        [-1.,  4.]])\n",
      "a = tensor([[1., 1.],\n",
      "        [2., 3.]])\n",
      "w =  tensor([[1.0000, 0.0000],\n",
      "        [0.2689, 0.7311]])\n",
      "out =  tensor([[-1.0000,  1.0000],\n",
      "        [-1.0000,  3.1932]])\n"
     ]
    }
   ],
   "source": [
    "xi = torch.tensor([[1, 0], [1, 1]]).float()\n",
    "qp = torch.eye(2).float()\n",
    "kp = torch.eye(2).float()\n",
    "kp[0,1] = 1\n",
    "vp = torch.tensor([[-1, 1], [0, 3]]).float()\n",
    "\n",
    "q = xi @ qp\n",
    "k = xi @ kp\n",
    "v = xi @ vp\n",
    "\n",
    "\n",
    "print('q = ', q)\n",
    "print('k = ', k)\n",
    "print('v = ', v)\n",
    "a = q @ k.T\n",
    "print('a =', a)\n",
    "w = a\n",
    "w[0, 1] = float('-inf')\n",
    "w = F.softmax(w, dim=-1)\n",
    "w\n",
    "print('w = ', w)\n",
    "out = w @ v\n",
    "print('out = ', out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62b5b3-9515-434c-b663-b46861a7d96d",
   "metadata": {},
   "source": [
    "### Implementing our Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b779aa4-8a83-418b-8442-068ed674ed16",
   "metadata": {},
   "source": [
    "Let's use these lessons to implement a transformer with self-attention. One last thing to mention: we add `Feed Forward` layers after our self-attention, which is basically just an MLP/vanilla neural network. This allows the model to further make any modifications to the representations that aren't dependent on other tokens. Also, another important thing to note: we can repeat this `self attention` process. A neural network can have *multiple* self-atn + FFN blocks stacked on top of each other. This allows the model to learn very complex relationships between tokens. For reference, GPT-3 had 96 attention blocks stacked on top of each other.\n",
    "\n",
    "We'll do 3 layers of attention + MLP in between each attention layer. The following implementation is courtesy of Andrej Karpathy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e2eafa-e8e3-428d-951c-23c34bf83c79",
   "metadata": {},
   "source": [
    "![](attn_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1243,
   "id": "f5eb8412-c5c7-4fe4-97a9-274f30ae6a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 64\n",
    "n_head = 1\n",
    "n_layer = 3\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C)\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out\n",
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        self.sa = Attention(n_embd)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class SimpleLMWithAttention(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx\n",
    "\n",
    "m = SimpleLMWithAttention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1244,
   "id": "d76837c4-351f-43bb-be13-eda1c711fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 4.651000022888184\n",
      "step 500 loss 2.0906076431274414\n",
      "step 1000 loss 1.9890193939208984\n",
      "step 1500 loss 1.9192143678665161\n",
      "step 2000 loss 1.8416926860809326\n",
      "step 2500 loss 1.782658338546753\n",
      "step 3000 loss 1.7743024826049805\n",
      "step 3500 loss 1.80558443069458\n",
      "step 4000 loss 1.8363431692123413\n",
      "step 4500 loss 1.789283037185669\n",
      "step 5000 loss 1.8259267807006836\n",
      "step 5500 loss 1.722832202911377\n",
      "step 6000 loss 1.7234731912612915\n",
      "step 6500 loss 1.7221698760986328\n",
      "step 7000 loss 1.755784034729004\n",
      "step 7500 loss 1.7279287576675415\n",
      "step 8000 loss 1.702441692352295\n",
      "step 8500 loss 1.7104332447052002\n",
      "step 9000 loss 1.6752803325653076\n",
      "step 9500 loss 1.6535974740982056\n",
      "1.6962977647781372\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "for steps in range(10000): # increase number of steps for good results... \n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # print(xb.shape)\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps%500 == 0:\n",
    "        print('step', steps, 'loss', loss.item())\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1245,
   "id": "123d642e-2150-4b2b-afe2-6069d2ab773e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GLOUCESTER:\n",
      "No, eyess,\n",
      "My lord, leaves, reasons foot a pare surment\n",
      "here upon plain,\n",
      "I we'll as 'tis that were whelp which thought lisand this then hears\n",
      "And of chils shame told on than ill England, let's heart set thouse for queen, 'Capul bade.\n",
      "\n",
      "DUKE Lady so be this petieung aluse cortain the bed: he tis age and her feel lain past of love quarry's bad it is a king-cursed:\n",
      "What the dound his bawd,\n",
      "To be oth Igent for most on pause, less from of uncle, are ussue!\n",
      "But, ands thee? Thou knave forew.\n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1246,
   "id": "504f79b1-0e96-433d-bcf8-7a658240c093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALYSSA LIU: thine hath infected me with Covid!\n",
      "\n",
      "LORD CASEY: thou art\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"ALYSSA LIU: thine hath infected me with Covid!\\n\\nLORD CASEY: thou art\"\n",
    "print(input_txt)\n",
    "ctx = encode(input_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1247,
   "id": "937b590d-1108-4436-a98b-2af3cdb58dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALYSSA LIU: thine hath infected me with Covid!\n",
      "\n",
      "LORD CASEY: thou art he more: there\n",
      "Here.\n",
      "\n",
      "KING RICHARD III:\n",
      "How what he wife trovice parous head you, faints poes\n",
      "Withose thee.\n",
      "In me that would\n",
      "Coments,\n",
      "Widly, she oath is eur law great was was to God my omble she perfices: what's in this groung'Thou to they cresolecess of a kingdom;\n",
      "Be, eve fape the repering sun that baulty beg: as please thee liege,\n",
      "Not mune shtience in more of head your garlet,\n",
      "As with is cide buld I may thou shalt shall not, to could Richard oncome, command lords. Come,\n",
      "A partle Citiing with \n"
     ]
    }
   ],
   "source": [
    "print(decode(m.generate(idx = torch.tensor(ctx).unsqueeze(0).long(), max_new_tokens=500)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1211,
   "id": "2f09bcaf-23a7-41da-af1f-85cdb171c25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss 1.6454037427902222\n",
      "step 500 loss 1.6876657009124756\n",
      "step 1000 loss 1.708350658416748\n",
      "step 1500 loss 1.685848355293274\n",
      "step 2000 loss 1.636243224143982\n",
      "step 2500 loss 1.6772477626800537\n",
      "step 3000 loss 1.6853232383728027\n",
      "step 3500 loss 1.7382113933563232\n",
      "step 4000 loss 1.6865171194076538\n",
      "step 4500 loss 1.7063449621200562\n",
      "step 5000 loss 1.7104920148849487\n",
      "step 5500 loss 1.7120356559753418\n",
      "step 6000 loss 1.6311984062194824\n",
      "step 6500 loss 1.6379395723342896\n",
      "step 7000 loss 1.7168763875961304\n",
      "step 7500 loss 1.6899621486663818\n",
      "step 8000 loss 1.681159496307373\n",
      "step 8500 loss 1.7122695446014404\n",
      "step 9000 loss 1.6832530498504639\n",
      "step 9500 loss 1.6917122602462769\n",
      "step 10000 loss 1.7628891468048096\n",
      "step 10500 loss 1.6706066131591797\n",
      "step 11000 loss 1.6901715993881226\n",
      "step 11500 loss 1.705620288848877\n",
      "step 12000 loss 1.6419752836227417\n",
      "step 12500 loss 1.684890866279602\n",
      "step 13000 loss 1.6462265253067017\n",
      "step 13500 loss 1.6590042114257812\n",
      "step 14000 loss 1.6221466064453125\n",
      "step 14500 loss 1.6231399774551392\n",
      "step 15000 loss 1.6877906322479248\n",
      "step 15500 loss 1.753821849822998\n",
      "step 16000 loss 1.66700279712677\n",
      "step 16500 loss 1.6816389560699463\n",
      "step 17000 loss 1.6505107879638672\n",
      "step 17500 loss 1.5927612781524658\n",
      "step 18000 loss 1.6304181814193726\n",
      "step 18500 loss 1.728723168373108\n",
      "step 19000 loss 1.6843644380569458\n",
      "step 19500 loss 1.6999449729919434\n",
      "1.6531990766525269\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
    "for steps in range(20000): # increase number of steps for good results... \n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train')\n",
    "    # print(xb.shape)\n",
    "    # evaluate the loss\n",
    "    logits, loss = m(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if steps%500 == 0:\n",
    "        print('step', steps, 'loss', loss.item())\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1213,
   "id": "f0b5700b-8a88-4e17-80fa-2bc8e7304a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALYSSA LIU: thine hath infected me with Covid!\n",
      "\n",
      "LORD CASEY: thou art that bled for grace pratorabused you, to the king, Warwick.\n",
      "Lawfull glass.\n",
      "I banished mine\n",
      "In live Edward make what war\n",
      "Done, not Roman.\n",
      "\n",
      "MONTAGUE:\n",
      "Ay, Anish his pardon, when rill-house. Thou show customb,\n",
      "Here is deal here; 'twoman,\n",
      "If why strokeuse your hand.\n",
      "But thou look the cause.\n",
      "\n",
      "EDWARD:\n",
      "You we may more\n",
      "Whom ensigal:\n",
      "And held garden.\n",
      "\n",
      "GLOUCESTER:\n",
      "The from Franced his misted\n",
      "As had corse.\n",
      "\n",
      "KING RICHArt it?\n",
      "Here whether action on us are am as oppother love\n",
      "Villain,\n",
      "Come, I am under well hourse: Hef what twent Green! Doth haughness do a, I say that. I would achieves?' an old not straitors,\n",
      "Lades the give does been, that your lust.\n",
      "\n",
      "BUCKINGHAM:\n",
      "My lord!\n",
      "Is not hap give you grain is though the\n",
      "life: I door:\n",
      "A\n",
      "moves, an ratelian's our tague go. In happy wolk one hold me save your as against thy was not with his sinued bless, as I be him, and DelphnishO, my enemy; gaves and whith these she kill her majesty\n",
      "And should the doubt,\n",
      "Such sins,\n",
      "By to husband death\n",
      "Diotle range,\n",
      "So the Caput\n"
     ]
    }
   ],
   "source": [
    "input_txt = \"ALYSSA LIU: thine hath infected me with Covid!\\n\\nLORD CASEY: thou art\"\n",
    "ctx = encode(input_txt)\n",
    "print(decode(m.generate(idx = torch.tensor(ctx).unsqueeze(0).long(), max_new_tokens=1000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7637d97d-269b-4ac6-a845-243dccfc2eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
